{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.data_pipeline import DataPipeline\n",
    "from src.models.model_factory import ModelFactory\n",
    "from src.training.base_trainer import BaseTrainer\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 85\n",
    "def seed_everything(seed):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_DIR = \"/home/karan/Documents/GitHub/BTP/torchmanager/configs\"\n",
    "DATA = \"data_no_aug.yaml\"\n",
    "MODEL = \"model_svit3.yaml\"\n",
    "TRAINER = \"training.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_config_path = os.path.join(CONFIG_DIR, DATA)\n",
    "data_pipeline = DataPipeline(data_config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, val_dataloader, test_dataloader = data_pipeline.data_loaders.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config_path = os.path.join(CONFIG_DIR, MODEL)\n",
    "model_factory = ModelFactory(model_config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_factory.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SqueezeViT_3(\n",
      "  (pooler_conv): ConvBlock(\n",
      "    (conv_bn_act): Sequential(\n",
      "      (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "      (1): SiLU()\n",
      "    )\n",
      "  )\n",
      "  (counter_pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (fire1): FireBlock(\n",
      "    (squeeze_block): Sequential(\n",
      "      (0): FireModule(\n",
      "        (squeeze): ConvBlock(\n",
      "          (conv_bn_act): Sequential(\n",
      "            (0): Conv2d(3, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (expand): ModuleDict(\n",
      "          (1x1): ConvBlock(\n",
      "            (conv_bn_act): Sequential(\n",
      "              (0): Conv2d(1, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            )\n",
      "          )\n",
      "          (3x3): ConvBlock(\n",
      "            (conv_bn_act): Sequential(\n",
      "              (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fire2): FireBlock(\n",
      "    (squeeze_block): Sequential(\n",
      "      (0): FireModule(\n",
      "        (squeeze): ConvBlock(\n",
      "          (conv_bn_act): Sequential(\n",
      "            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (expand): ModuleDict(\n",
      "          (1x1): ConvBlock(\n",
      "            (conv_bn_act): Sequential(\n",
      "              (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            )\n",
      "          )\n",
      "          (3x3): ConvBlock(\n",
      "            (conv_bn_act): Sequential(\n",
      "              (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): FireModule(\n",
      "        (squeeze): ConvBlock(\n",
      "          (conv_bn_act): Sequential(\n",
      "            (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (expand): ModuleDict(\n",
      "          (1x1): ConvBlock(\n",
      "            (conv_bn_act): Sequential(\n",
      "              (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            )\n",
      "          )\n",
      "          (3x3): ConvBlock(\n",
      "            (conv_bn_act): Sequential(\n",
      "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): FireModule(\n",
      "        (squeeze): ConvBlock(\n",
      "          (conv_bn_act): Sequential(\n",
      "            (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (expand): ModuleDict(\n",
      "          (1x1): ConvBlock(\n",
      "            (conv_bn_act): Sequential(\n",
      "              (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            )\n",
      "          )\n",
      "          (3x3): ConvBlock(\n",
      "            (conv_bn_act): Sequential(\n",
      "              (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (svit1): SqueezeViTBlock(\n",
      "    (fire1): FireBlock(\n",
      "      (squeeze_block): Sequential(\n",
      "        (0): FireModule(\n",
      "          (squeeze): ConvBlock(\n",
      "            (conv_bn_act): Sequential(\n",
      "              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): SiLU()\n",
      "            )\n",
      "          )\n",
      "          (expand): ModuleDict(\n",
      "            (1x1): ConvBlock(\n",
      "              (conv_bn_act): Sequential(\n",
      "                (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              )\n",
      "            )\n",
      "            (3x3): ConvBlock(\n",
      "              (conv_bn_act): Sequential(\n",
      "                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): FireModule(\n",
      "          (squeeze): ConvBlock(\n",
      "            (conv_bn_act): Sequential(\n",
      "              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): SiLU()\n",
      "            )\n",
      "          )\n",
      "          (expand): ModuleDict(\n",
      "            (1x1): ConvBlock(\n",
      "              (conv_bn_act): Sequential(\n",
      "                (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              )\n",
      "            )\n",
      "            (3x3): ConvBlock(\n",
      "              (conv_bn_act): Sequential(\n",
      "                (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (encoder): Sequential(\n",
      "      (0): EncoderBlock(\n",
      "        (layer_norm_1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "        (mhsa): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (layer_norm_2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (mlp): Sequential(\n",
      "            (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "            (4): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): EncoderBlock(\n",
      "        (layer_norm_1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "        (mhsa): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (layer_norm_2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (mlp): Sequential(\n",
      "            (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "            (4): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): EncoderBlock(\n",
      "        (layer_norm_1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "        (mhsa): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (layer_norm_2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (mlp): Sequential(\n",
      "            (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "            (4): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): EncoderBlock(\n",
      "        (layer_norm_1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "        (mhsa): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (layer_norm_2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (mlp): Sequential(\n",
      "            (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "            (4): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (conv3): ConvBlock(\n",
      "      (conv_bn_act): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "    )\n",
      "    (fire2): FireBlock(\n",
      "      (squeeze_block): Sequential(\n",
      "        (0): FireModule(\n",
      "          (squeeze): ConvBlock(\n",
      "            (conv_bn_act): Sequential(\n",
      "              (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): SiLU()\n",
      "            )\n",
      "          )\n",
      "          (expand): ModuleDict(\n",
      "            (1x1): ConvBlock(\n",
      "              (conv_bn_act): Sequential(\n",
      "                (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              )\n",
      "            )\n",
      "            (3x3): ConvBlock(\n",
      "              (conv_bn_act): Sequential(\n",
      "                (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fire3): FireBlock(\n",
      "    (squeeze_block): Sequential(\n",
      "      (0): FireModule(\n",
      "        (squeeze): ConvBlock(\n",
      "          (conv_bn_act): Sequential(\n",
      "            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (expand): ModuleDict(\n",
      "          (1x1): ConvBlock(\n",
      "            (conv_bn_act): Sequential(\n",
      "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            )\n",
      "          )\n",
      "          (3x3): ConvBlock(\n",
      "            (conv_bn_act): Sequential(\n",
      "              (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (conv): ConvBlock(\n",
      "    (conv_bn_act): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU()\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=256, out_features=14, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "535,426 total parameters.\n"
     ]
    }
   ],
   "source": [
    "# print total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'{total_params:,} total parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierTrainer(BaseTrainer):\n",
    "    def __init__(self, train_config_path):\n",
    "        super().__init__(train_config_path)\n",
    "    \n",
    "    def train_step(self, batch_data, batch_idx):\n",
    "        # init_batch\n",
    "        inputs, targets = batch_data\n",
    "\n",
    "        # process_batch\n",
    "        outputs = self.model(inputs)\n",
    "\n",
    "        # compute_loss\n",
    "        loss_value = self.loss(outputs, targets)\n",
    "\n",
    "        # compute_metrics\n",
    "        metrics_values = {}\n",
    "        for metric_name, metric in self.metrics.items():\n",
    "            metric_value = metric(outputs, targets)\n",
    "            metrics_values[metric_name] = metric_value\n",
    "        \n",
    "        # end_batch\n",
    "        results = {\n",
    "            'loss' : loss_value,\n",
    "            **metrics_values\n",
    "        }\n",
    "\n",
    "        return results\n",
    "    \n",
    "    def val_step(self, batch_data, batch_idx):\n",
    "        return self.train_step(batch_data, batch_idx)\n",
    "    \n",
    "    def test_step(self, batch_data, batch_idx):\n",
    "        return self.train_step(batch_data, batch_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config_path = os.path.join(CONFIG_DIR, TRAINER)\n",
    "training_manager = ClassifierTrainer(training_config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 14])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# make dataloaders dict with only train and val\u001b[39;00m\n\u001b[1;32m      2\u001b[0m train_dataloaders \u001b[39m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m: train_dataloader,\n\u001b[1;32m      4\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m: val_dataloader\n\u001b[1;32m      5\u001b[0m }\n\u001b[0;32m----> 7\u001b[0m train_history \u001b[39m=\u001b[39m training_manager\u001b[39m.\u001b[39;49mfit(model, train_dataloaders)\n",
      "File \u001b[0;32m~/Documents/GitHub/BTP/torchmanager/src/training/base_trainer.py:109\u001b[0m, in \u001b[0;36mBaseTrainer.fit\u001b[0;34m(self, model, dataloaders)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39mif\u001b[39;00m phase \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    107\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 109\u001b[0m results \u001b[39m=\u001b[39m strategy(batch_data, batch_idx)\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m phase \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    112\u001b[0m     results[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[11], line 13\u001b[0m, in \u001b[0;36mClassifierTrainer.train_step\u001b[0;34m(self, batch_data, batch_idx)\u001b[0m\n\u001b[1;32m     10\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(inputs)\n\u001b[1;32m     12\u001b[0m \u001b[39m# compute_loss\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m loss_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss(outputs, targets)\n\u001b[1;32m     15\u001b[0m \u001b[39m# compute_metrics\u001b[39;00m\n\u001b[1;32m     16\u001b[0m metrics_values \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/miniconda3/envs/BTP_Proj/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/BTP_Proj/lib/python3.11/site-packages/torch/nn/modules/loss.py:619\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 619\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m~/miniconda3/envs/BTP_Proj/lib/python3.11/site-packages/torch/nn/functional.py:3089\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3087\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3088\u001b[0m \u001b[39mif\u001b[39;00m target\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize():\n\u001b[0;32m-> 3089\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   3090\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing a target size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) that is different to the input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) is deprecated. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3091\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease ensure they have the same size.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m   3092\u001b[0m     )\n\u001b[1;32m   3094\u001b[0m \u001b[39mif\u001b[39;00m weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3095\u001b[0m     new_size \u001b[39m=\u001b[39m _infer_size(target\u001b[39m.\u001b[39msize(), weight\u001b[39m.\u001b[39msize())\n",
      "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 14])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "# make dataloaders dict with only train and val\n",
    "train_dataloaders = {\n",
    "    'train': train_dataloader,\n",
    "    'val': val_dataloader\n",
    "}\n",
    "\n",
    "train_history = training_manager.fit(model, train_dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': {'loss': [tensor(1.8957, device='cuda:0')],\n",
       "  'multiLabelAccuracy': [0.0]}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_manager.test(model, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_manager_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
